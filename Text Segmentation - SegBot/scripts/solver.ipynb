{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eacb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import random\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "import copy\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_decoder_index_XY(batchY):\n",
    "    '''\n",
    "\n",
    "    :param batchY: like [0 0 1 0 0 0 0 1]\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "\n",
    "    returnX =[]\n",
    "    returnY =[]\n",
    "    for i in range(len(batchY)):\n",
    "\n",
    "        curY = batchY[i]\n",
    "\n",
    "        index_1 = np.where(curY==1)\n",
    "\n",
    "\n",
    "        decoderY = index_1[0]\n",
    "\n",
    "        if len(index_1[0]) ==1:\n",
    "            decoderX = np.array([0])\n",
    "        else:\n",
    "            decoderX = np.append([0],decoderY[0:-1]+1)\n",
    "\n",
    "        returnX.append(decoderX)\n",
    "        returnY.append(decoderY)\n",
    "\n",
    "    returnX = np.array(returnX)\n",
    "    returnY = np.array(returnY)\n",
    "\n",
    "    return returnX,returnY\n",
    "\n",
    "def align_variable_numpy(X,maxL,paddingNumber):\n",
    "\n",
    "    aligned = []\n",
    "    for cur in X:\n",
    "        ext_cur = []\n",
    "        ext_cur.extend(cur)\n",
    "        ext_cur.extend([paddingNumber] * (maxL - len(cur)))\n",
    "        aligned.append(ext_cur)\n",
    "    aligned = np.array(aligned)\n",
    "\n",
    "    return aligned\n",
    "\n",
    "\n",
    "def sample_a_sorted_batch_from_numpy(numpyX,numpyY,batch_size,use_cuda):\n",
    "\n",
    "\n",
    "    if batch_size != None:\n",
    "        select_index = random.sample(range(len(numpyY)), batch_size)\n",
    "    else:\n",
    "        select_index = np.array(range(len(numpyY)))\n",
    "\n",
    "    batch_x = copy.deepcopy(numpyX[select_index])\n",
    "    batch_y = copy.deepcopy(numpyY[select_index])\n",
    "\n",
    "    index_decoder_X,index_decoder_Y = get_decoder_index_XY(batch_y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    all_lens = np.array([len(x) for x in batch_y])\n",
    "    maxL = np.max(all_lens)\n",
    "\n",
    "    idx = np.argsort(all_lens)\n",
    "    idx = idx[::-1]  # decreasing\n",
    "\n",
    "    batch_x = batch_x[idx]\n",
    "    batch_y = batch_y[idx]\n",
    "    all_lens = all_lens[idx]\n",
    "\n",
    "    index_decoder_X = index_decoder_X[idx]\n",
    "    index_decoder_Y = index_decoder_Y[idx]\n",
    "\n",
    "\n",
    "    numpy_batch_x = batch_x\n",
    "\n",
    "\n",
    "\n",
    "    batch_x = align_variable_numpy(batch_x,maxL,0)\n",
    "    batch_y = align_variable_numpy(batch_y,maxL,2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    batch_x = Variable(torch.from_numpy(batch_x.astype(np.int64)))\n",
    "\n",
    "\n",
    "\n",
    "    if use_cuda:\n",
    "        batch_x = batch_x.cuda()\n",
    "\n",
    "\n",
    "\n",
    "    return  numpy_batch_x,batch_x,batch_y,index_decoder_X,index_decoder_Y,all_lens,maxL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TrainSolver(object):\n",
    "    def __init__(self, model,train_x,train_y,dev_x,dev_y,save_path,batch_size,eval_size,epoch, lr,lr_decay_epoch,weight_decay,use_cuda):\n",
    "\n",
    "        self.lr = lr\n",
    "        self.model = model\n",
    "        self.epoch = epoch\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.use_cuda = use_cuda\n",
    "        self.batch_size = batch_size\n",
    "        self.lr_decay_epoch = lr_decay_epoch\n",
    "        self.eval_size  = eval_size\n",
    "\n",
    "\n",
    "        self.dev_x, self.dev_y = dev_x, dev_y\n",
    "\n",
    "        self.model = model\n",
    "        self.save_path = save_path\n",
    "        self.weight_decay =weight_decay\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def sample_dev(self):\n",
    "\n",
    "        select_index = random.sample(range(len(self.train_y)),self.eval_size)\n",
    "\n",
    "        test_tr_x = self.train_x[select_index]\n",
    "        test_tr_y = self.train_y[select_index]\n",
    "\n",
    "        return test_tr_x,test_tr_y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_batch_micro_metric(self,pre_b, ground_b):\n",
    "\n",
    "\n",
    "\n",
    "        All_C = []\n",
    "        All_R = []\n",
    "        All_G = []\n",
    "        for i,cur_seq_y in enumerate(ground_b):\n",
    "            index_of_1 = np.where(cur_seq_y==1)[0]\n",
    "            index_pre = pre_b[i]\n",
    "\n",
    "\n",
    "\n",
    "            index_pre = np.array(index_pre)\n",
    "            END_B = index_of_1[-1]\n",
    "            index_pre = index_pre[index_pre != END_B]\n",
    "            index_of_1 = index_of_1[index_of_1 != END_B]\n",
    "\n",
    "            no_correct = len(np.intersect1d(list(index_of_1), list(index_pre)))\n",
    "            All_C.append(no_correct)\n",
    "            All_R.append(len(index_pre))\n",
    "            All_G.append(len(index_of_1))\n",
    "\n",
    "\n",
    "        return All_C,All_R,All_G\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_batch_metric(self,pre_b, ground_b):\n",
    "\n",
    "        b_pr =[]\n",
    "        b_re =[]\n",
    "        b_f1 =[]\n",
    "        for i,cur_seq_y in enumerate(ground_b):\n",
    "            index_of_1 = np.where(cur_seq_y==1)[0]\n",
    "            index_pre = pre_b[i]\n",
    "\n",
    "            no_correct = len(np.intersect1d(index_of_1,index_pre))\n",
    "\n",
    "            cur_pre = no_correct / len(index_pre)\n",
    "            cur_rec = no_correct / len(index_of_1)\n",
    "            cur_f1 = 2*cur_pre*cur_rec/ (cur_pre+cur_rec)\n",
    "\n",
    "            b_pr.append(cur_pre)\n",
    "            b_re.append(cur_rec)\n",
    "            b_f1.append(cur_f1)\n",
    "\n",
    "        return b_pr,b_re,b_f1\n",
    "\n",
    "\n",
    "\n",
    "    def check_accuracy(self,dataX,dataY):\n",
    "\n",
    "\n",
    "        need_loop = int(np.ceil(len(dataY) / self.batch_size))\n",
    "\n",
    "        all_ave_loss =[]\n",
    "        all_boundary =[]\n",
    "        all_boundary_start = []\n",
    "        all_align_matrix = []\n",
    "        all_index_decoder_y =[]\n",
    "        all_x_save = []\n",
    "\n",
    "        all_C =[]\n",
    "        all_R =[]\n",
    "        all_G =[]\n",
    "        for lp in range(need_loop):\n",
    "            startN = lp*self.batch_size\n",
    "            endN =  (lp+1)*self.batch_size\n",
    "            if endN > len(dataY):\n",
    "                endN = len(dataY)\n",
    "\n",
    "            numpy_batch_x, batch_x, batch_y, index_decoder_X, index_decoder_Y, all_lens, maxL = sample_a_sorted_batch_from_numpy(\n",
    "                dataX[startN:endN], dataY[startN:endN], None, self.use_cuda)\n",
    "\n",
    "\n",
    "            batch_ave_loss, batch_boundary, batch_boundary_start, batch_align_matrix = self.model.predict(batch_x,\n",
    "                                                                                                      index_decoder_Y,\n",
    "                                                                                                  all_lens)\n",
    "\n",
    "            all_ave_loss.extend([batch_ave_loss.data])  #[batch_ave_loss.data[0]]\n",
    "            all_boundary.extend(batch_boundary)\n",
    "            all_boundary_start.extend(batch_boundary_start)\n",
    "            all_align_matrix.extend(batch_align_matrix)\n",
    "            all_index_decoder_y.extend(index_decoder_Y)\n",
    "            all_x_save.extend(numpy_batch_x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ba_C,ba_R,ba_G = self.get_batch_micro_metric(batch_boundary,batch_y)\n",
    "\n",
    "            all_C.extend(ba_C)\n",
    "            all_R.extend(ba_R)\n",
    "            all_G.extend(ba_G)\n",
    "\n",
    "\n",
    "        ba_pre = np.sum(all_C)/ np.sum(all_R)\n",
    "        ba_rec = np.sum(all_C)/ np.sum(all_G)\n",
    "        ba_f1 = 2*ba_pre*ba_rec/ (ba_pre+ba_rec)\n",
    "\n",
    "\n",
    "        return np.mean(all_ave_loss),ba_pre,ba_rec,ba_f1, (all_x_save,all_index_decoder_y,all_boundary, all_boundary_start, all_align_matrix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def adjust_learning_rate(self,optimizer,epoch,lr_decay=0.5, lr_decay_epoch=50):\n",
    "\n",
    "        if (epoch % lr_decay_epoch == 0) and (epoch != 0):\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= lr_decay\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        self.test_train_x, self.test_train_y = self.sample_dev()\n",
    "\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "\n",
    "\n",
    "        num_each_epoch = int(np.round(len(self.train_y) / self.batch_size))\n",
    "\n",
    "        os.mkdir(self.save_path)\n",
    "\n",
    "        best_i =0\n",
    "        best_f1 =0\n",
    "\n",
    "        for epoch in range(self.epoch):\n",
    "\n",
    "            self.adjust_learning_rate(optimizer, epoch, 0.8, self.lr_decay_epoch)\n",
    "\n",
    "            track_epoch_loss = []\n",
    "            for iter in range(num_each_epoch):\n",
    "                print(\"epoch:%d,iteration:%d\" % (epoch, iter))\n",
    "\n",
    "                numpy_batch_x,batch_x, batch_y, index_decoder_X, index_decoder_Y, all_lens, maxL = sample_a_sorted_batch_from_numpy(\n",
    "                    self.train_x, self.train_y, self.batch_size, self.use_cuda)\n",
    "\n",
    "                self.model.zero_grad()\n",
    "\n",
    "                neg_loss = self.model.neg_log_likelihood(batch_x, index_decoder_X, index_decoder_Y,all_lens)\n",
    "\n",
    "\n",
    "\n",
    "                neg_loss_v = float(neg_loss.data[0])\n",
    "                print(neg_loss_v)\n",
    "                track_epoch_loss.append(neg_loss_v)\n",
    "\n",
    "                neg_loss.backward()\n",
    "\n",
    "                clip_grad_norm(self.model.parameters(), 5)\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "            #TODO: after each epoch,check accuracy\n",
    "\n",
    "\n",
    "            self.model.eval()\n",
    "\n",
    "            tr_batch_ave_loss, tr_pre, tr_rec, tr_f1 ,visdata=    self.check_accuracy(self.test_train_x,self.test_train_y)\n",
    "\n",
    "            dev_batch_ave_loss, dev_pre, dev_rec, dev_f1, visdata =self.check_accuracy(self.dev_x,self.dev_y)\n",
    "            print()\n",
    "\n",
    "            if best_f1 < dev_f1:\n",
    "                best_f1 = dev_f1\n",
    "                best_rec = dev_rec\n",
    "                best_pre = dev_pre\n",
    "                best_i = epoch\n",
    "\n",
    "\n",
    "\n",
    "            save_data = [epoch,tr_batch_ave_loss,tr_pre,tr_rec,tr_f1,\n",
    "                         dev_batch_ave_loss,dev_pre,dev_rec,dev_f1]\n",
    "\n",
    "\n",
    "            save_file_name = 'bs_{}_es_{}_lr_{}_lrdc_{}_wd_{}_epoch_loss_acc_pk_wd.txt'.format(self.batch_size,self.eval_size,self.lr,self.lr_decay_epoch,self.weight_decay)\n",
    "            with open(os.path.join(self.save_path,save_file_name), 'a') as f:\n",
    "                f.write(','.join(map(str,save_data))+'\\n')\n",
    "\n",
    "\n",
    "            if epoch % 1 ==0 and epoch !=0:\n",
    "                torch.save(self.model, os.path.join(self.save_path,r'model_epoch_%d.torchsave'%(epoch)))\n",
    "\n",
    "\n",
    "            self.model.train()\n",
    "\n",
    "        return best_i,best_pre,best_rec,best_f1\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
